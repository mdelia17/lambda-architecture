version: '3'

networks:
  net-lambda:
    name: net-lambda
    driver: bridge
  net-kafka:
    external:
      name: net-kafka
  net-cassandra:
    external:
      name: net-cassandra


services:    
  spark-master:
    image: bde2020/spark-master:3.1.1-hadoop3.2
    container_name: spark-master
    restart: on-failure
    ports:
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
    networks:
      - net-lambda
      - net-cassandra
      - net-kafka
    volumes:
      - './streaming:/streaming:ro'
      - './batch:/batch:ro'
      - './spark-cassandra-connector-assembly-3.0.1-5-g8bab1061.jar:/spark/jars/spark-cassandra-connector-assembly-3.0.1-5-g8bab1061.jar:ro'

  spark-worker-1:
    image: bde2020/spark-worker:3.1.1-hadoop3.2
    container_name: spark-worker-1
    restart: on-failure
    depends_on:
      - spark-master
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
    networks:
      - net-lambda
      - net-cassandra
      - net-kafka

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    # restart: on-failure
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - CLUSTER_NAME=test
    ports:
      - 9870:9870
      - 8020:8020
    networks:
      - net-lambda

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: on-failure
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    ports:
      - 50075:50075
      - 50010:50010
    networks:
      - net-lambda

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    restart: on-failure
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    ports:
      - 50076:50076
      - 50011:50011
    networks:
      - net-lambda

  kafka-connect-cuns:
    image: confluentinc/cp-kafka-connect-base:6.0.1
    container_name: kafka-connect-cuns
    ports:
      - 9067:9067
    networks:
      - net-lambda
      - net-kafka
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "broker:29092"
      CONNECT_REST_PORT: 9067
      CONNECT_GROUP_ID: compose-connect-group-cuns
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs-cuns
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets-cuns
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status-cuns
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect-cuns"
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_PLUGIN_PATH: '/usr/share/java,/usr/share/confluent-hub-components/'
    command: 
      - bash 
      - -c 
      - |
        echo "Installing connector plugins"
        confluent-hub install --no-prompt confluentinc/kafka-connect-hdfs3:1.1.1
        #
        # -----------
        # Launch the Kafka Connect worker
        /etc/confluent/docker/run &
        #
        # Don't exit
        sleep infinity